\documentclass[9pt,twocolumn,twoside]{../../styles/osajnl}
\usepackage{fancyvrb}
\journal{i524}

\title{Naiad}

\author[1,*]{Rahul Raghatate}
\author[1]{Snehal Chemburkar}

\affil[1]{School of Informatics and Computing, Bloomington, IN 47408,
  U.S.A.}

\affil[*]{Corresponding authors: rragahta@iu.edu, snehchem@iu.edu }

\dates{paper-2, \today}

\ociscodes{Naiad, iterative, dataflow, graph, stream}

% replace this with your url in github/gitlab
\doi{\url{https://github.com/cloudmesh/sp17-i524/blob/master/paper2/S17-IR-2026/report.pdf}}


\begin{abstract}
Naiad is a distributed system based on computational model called
timely dataflow developed for execution of data-parallel, cyclic
dataflow programs. It provides an in-memory distributed dataflow
framework which exposes control over data partitioning and enables
features like the high throughput of batch processors, the low latency
of stream processors, and the ability to perform iterative and
incremental computations. These features allow the efficient
implementation of many dataflow patterns, from bulk and streaming
computation to iterative graph processing and machine learning.  This
paper explains the Naiad Framework, its abstractions, and the
reasoning behind it.
\newline
\end{abstract}

\setboolean{displaycopyright}{true}

\begin{document}

\maketitle

\section{Introduction}

Abundance of distributed dataflow systems around the big data
ecosystem has resulted in achieving many goals like high throughput,
incremental updating, low latency stream processing, iterative
computation. An iterative computation can be think of as some function
being looped on its output iteratively until there are no changes
between the input and the output and the function converges to a fixed
point. On the other hand, in incremental processing, we start with
initial input $A_{0}$ and produce some output $B_0$. At some later
point, a change $\delta A_1$ to the original input $A_0$, leads to new
input $A_1 = A_0 + \delta A_1$. Incremental model produces an
incremental update to the output, so $\delta B_1 = F(\delta A_1)$ and
$B_1 = \delta B_1 + B_0$. Next state computation in incremental model
is based on only previous state (i.e. A0 and B0)
\cite{www-blog-naiad-summary}.

Many data processing tasks require low-latency interactive access to
results, iterative sub-computations, and consistent intermediate
outputs so that sub-computations can be nested and composed. Most
data-parallel systems support either iterative or incremental
computations with the dataflow model, but not both. Frameworks like
descendants of MapReduce \cite{paper-incoop,paper-nectar},stream
processing systems \cite{eventstream,paper-spade}, materialized
view-maintenance engines \cite{paper-maintainance}, provide efficient
support for incremental input, but do not support iterative
processing. On the other hand, iterative data-parallel frameworks like
Datalog \cite{paper-datalog}, recursive SQL databases \cite{paper-SQL}
and systems adding iteration over MapReduce like model
\cite{paper-haloop,paper-twister,paper-ceil,paper-RDD} provide a
constrained programming model (e.g. stratified negation in Datalog)
and none supports incremental input processing \cite{paper2-Naiad}.

With a goal to find common low-level abstraction and design general
purpose system which resolves above mentioned issues in computation
workloads, Naiad, a timely dataflow system was developed at Microsoft
by Derek G. Murray \textit{et al.}.

Naiad provides a distributed platform for executing data parallel
cyclic dataflow programs. It offers high throughput batch processing,
low latency stream processing and the ability to perform iterative and
incremental computations all in one framework
\cite{www-blog-huula-review-naiad}.

In Naiad, the underlying timely dataflow computation framework
provides a mechanism for iteration and incremental updates. Moreover,
the high-level language support provides a structured means of
expressing these computations. This framework can be utilized to build
many expressive programming model on top of Naiad’s low-level
primitives enabling such diverse tasks as streaming data analysis,
iterative machine learning, and interactive graph mining
\cite{paper2-Naiad}.


\section{Architecture}

The Naiad architecture consists of two main components- (1)
incremental processing of incoming updates and (2) low-latency
real-time querying of the application state.
\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/Naiad_application.JPG}}
\caption{Naiad Application that support real time queries on
  continually updated data \cite{paper1-Naiad}.}
\label{Naiad-arch}
\end{figure}

Figure \ref{Naiad-arch} shows a Naiad application that supports
real-time queries on continually updated data. The dashed rectangle
represents iterative processing that incrementally updates as new data
arrive \cite{paper1-Naiad}.

From Figure \ref{Naiad-arch}, query path is clearly separated from the
update path. This results in query processing separately on a slightly
stale version of the current application state and the query path does
not get blocked or incur delays due to the update processing. This
also resolves complex situations: If queries shared the same path with
updates, the queries could be accessing partially
processed/incomplete/inconsistent states, which would have to be taken
care out separately.

Even though hybrid approach to assemble the application in Figure
\ref{Naiad-arch} by combining multiple existing systems have been
widely deployed, applications built on a single platform as in Figure
\ref{Naiad-arch} are typically more efficient, succinct, and
maintainable \cite{paper1-Naiad}.

\section{Timely dataflow}
Applications should produce consistent results, and consistency
requires coordination, both across dataflow nodes and around loops
\cite{paper1-Naiad}. Timely dataflow is a computational model that
attaches virtual timestamps to events in structured cyclic data-flow
graphs providing coordination mechanism that allows low-latency
asynchronous message processing while efficiently tracking global
progress and synchronizing only where necessary to enforce
consistency. Naiad model simply checkpoints its state periodically,
restoring the entire system state to the most recent checkpoint on
failure. Even if it is not a sophisticated design, it was chosen in
part for its low overhead. Faster common-case processing allows more
computation to take place in the intervals between checkpointing, and
thus, often decreases the total time to job completion
\cite{paper3-Naiad}.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/timely-dataflow-graph.jpg}}
\caption{A simple timely dataflow graph \cite{paper1-Naiad}.}
\label{timely-dataflow-graph}
\end{figure}

Consider a simple timely dataflow graph shown in Figure
\ref{timely-dataflow-graph} showing a loop context nested in the
top-level streaming context. Vertices A and D are not in any loop
context resulting in no loop counters for their timestamps. Whereas,
vertices B, C, and F are nested in a single loop context, so their
timestamps have a single loop counter. The Ingress (I) and Egress (E)
vertices sit on the boundary of a loop context. They monitor the loop
by adding and removing loop counters to and from timestamps as
messages pass through them.

Naiad being a timely dataflow computational model, utilizes similar to
above mentioned computational process and provide the basis for an
efficient, lightweight coordination mechanism. Timely dataflow
supports the following three features:

\begin{enumerate}
\item Structured loops allowing feedback in the dataflow
\item Stateful dataflow vertices capable of consuming and producing
  records without global coordination, and
\item Notifications for vertices once they have received all records
  for a given round of input or loop iteration.
\end{enumerate}

Structured loops and Stateful dataflow vertices allows iterative and
incremental computations with low latency. Notifications makes Naiad
possible to produce consistent results, at both outputs and
intermediate stages of computations, in the presence of streaming or
iteration \cite{paper1-Naiad}. The timely dataflow in Naiad is
achieved by optimization in services like asynchronous messaging,
iterative dataflow, progress tracking and consistency improvisation.

\subsection{Asynchronous messaging}
All dataflow models require some communication means for message
passing between node over outgoing edges. In a timely dataflow system,
each node implements an \textit{OnRecv} event handler that the system
can call when a message arrives on an incoming edge, and the system
provides a \textit{Sendmethod} that a node can invoke from any of its
event handlers to send a message on an outgoing edge
\cite{paper3-Naiad}. Messages are delivered asynchronously.

\subsection{Consistency}
Computations like reduction functions \textit{Count} or
\textit{Average} include subroutines that must accumulate all of their
input before generating an output. At the same time, distributed
applications commonly split input into small asynchronous messages to
reduce latency and buffering. For timely dataflow to support
incremental computations on unbounded streams of input as well as
iteration, it has a mechanism to signal when a node (or data-parallel
set of nodes) has seen a consistent subset of the input for which to
produce a result \cite{paper3-Naiad}.

\subsection{Iterative Graph Dataflow}
A Naiad dataflow graph is acyclic apart from structurally nested
cycles that correspond to loops in the program. The logical timestamp
associated with each event represents the batch of input that the
event is associated with, and each subsequent integer gives the
iteration count of any (nested) loops that contain the node. Every
path around a cycle includes a special node that increments the
innermost coordinate of the timestamp.  The system enforced rule
restricts event handler from sending a message with a time earlier
than the timestamp for the event it is handling ensuring a
\textit{partial order} on all of the pending events (undelivered
messages and notifications) in the system, thus enabling efficient
progress tracking \cite{paper1-Naiad}.

\subsection{Progress Tracking}
The ability to deliver notifications (an event that fires when all
messages at or before a particular logical timestamp have been
delivered to a particular node) promptly and safely is critical. This
provides timely dataflow system an ability to support low-latency
incremental and iterative computation with consistent results. Naiad
can implement progress trackers to establish the guarantee that no
more messages with a particular timestamp can be sent to a node
\cite{paper1-Naiad}.

\section{Achieving Timely dataflow}
Each event has a timestamp and a location (either a vertex or edge),
and are referred as pointstamp. The timely dataflow graph structure
ensures that for any locations $l_1$ and $l_2$ connected by two paths
with different summaries, one of the path summaries always yields
adjusted timestamps earlier than the other. For each pair $l_1$ and
$l_2$, we find the minimal path summary over all paths from $l_1$ to
$l_2$ using a straightforward graph propagation algorithm, and record
it as $\Psi[l_1, l_2]$. To efficiently evaluate the possible relation
for two pointstamps $(t_1, l_1)$ and $(t_2, l_2)$, we test whether
$\Psi[l_1, l_2](t_1) \le t_2$ \cite{paper1-Naiad}.

Informally, Timely Dataflow supports directed dataflow graphs with
structured cycles which is analogous to structured loops in a standard
imperative programming language. This structure provides information
about where records might possibly flow in the computation, allowing
an implementation like Naiad to efficiently track and inform dataflow
vertexes about the possibility of additional records arriving at given
streaming epochs or iterations \cite{www-naiad}.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/Naiad-Logical_dataflow.JPG}}
\caption{The mapping of a logical dataflow graph onto the distributed
  Naiad system architecture \cite{paper1-Naiad}.}
\label{Naiad-LDF}
\end{figure}

\section{System Implementation}
Naiad is high-performance distributed implementation of timely
dataflow and is written in C$\#$, and runs on Windows, Linux, and Mac
OS. Figure \ref{Naiad-LDF} shows the schematic architecture of a Naiad
cluster consisting a group of processes hosting workers that manage a
partition of the timely dataflow vertices \cite{paper3-Naiad}. Each
worker may host several stages of the dataflow graph. The workers are
data nodes and keep a portion of the input data (usually a large-scale
input graph, such as Twitter follower graph) in memory. So it makes
sense to move computation (dataflow graph stages) to the data (the
partitioned input
graph)\cite{www-informationage-blog-wordpress-naiad}. Each process
participates in a distributed progress tracking protocol, in order to
coordinate the delivery of notifications.  All the features of C$\#$,
including classes, structs, and lambda functions, to build a timely
dataflow graph from a system-provided library of generic Stream
objects can be implemented. Naiad model uses deferred execution
method: like adding a node to internal data flow graph at runtime
while executing a method like Max on a Stream \cite{paper1-Naiad}.

Naiad's distributed implementation also exhibits features like
distributed progress tracking, a simple but extensible implementation
of fault tolerance, high availability, avoidance of micro-straggler
(events like packet loss, contention on concurrent data structures,
and garbage collection which leads to delays ranging from tens of
milliseconds to tens of seconds) which is main obstacle to scalability
for low-latency workloads \cite{paper1-Naiad}.

\section{Performance Evaluation}
Naiad's performance over supporting high-throughput, low latency,
data-parallelism, batch processing, iterative graph data processing
have been examined using several notable microbenchmarks by Murray
\textit{et al.} \cite{paper1-Naiad} such as Throughput, Latency,
Protocol Optimizations, Scaling.

The hardware configuration consists of two racks of 32 computers, each
with two quad-core 2.1 GHz AMD Opteron processors, 16 GB of memory,
and an Nvidia NForce Gigabit Ethernet NIC. Also rack switches have 40
Gbps uplink to the core switch. Average across five trials are
plotted, with error bars showing minimum and maximum values
\cite{paper1-Naiad}.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/throughput.jpg}}
\caption{ Evaluation of Naiad system throughput rate on synthetic
  dataset \cite{paper1-Naiad}.}
\label{throughput}
\end{figure}

\subsection{Throughput}
The Naiad program constructs a cyclic dataflow that repeatedly
performs the all-to-all data exchange of a fixed number of records.
Figure \ref{throughput} plots the aggregate throughput against the
number of computers with uppermost line as Ideal throughput, middle
one depicting achievable throughput given network topology, TCP
overheads, and .NET API costs. The final line shows the throughput
that Naiad achieves for a large number of 8-byte records (50M per
computer) exchanges between all processes in the cluster.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/latency.jpg}}
\caption{ Evaluation of Naiad system global barrier latency for
  synthetic dataset \cite{paper1-Naiad}.}
\label{latency}
\end{figure}

\subsection{Latency}
Latency microbenchmark evaluates the minimal time required for global
coordination. Figure \ref{latency} plots the distribution of times for
100K iterations using median, quartiles, and $95^{th}$ percentile
values indicating low median time per iteration and adverse impact at
$95^{th}$ percentile mark due to increased number of computers.


\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/scaling1.jpg}}
\caption{ Strong Scaling microbenchmark evaluation for Naiad system
  over synthetic dataset \cite{paper1-Naiad}.}
\label{strong scaling}
\end{figure}

\subsection{Scaling}
Based on input size and resource availability, strong and weak scaling
microbenchmarks are achieved. Keeping input fixed and increased
compute resources, running time as shown in Figure \ref{strong
  scaling} has been plotted for two applications, WordCount and Weakly
connected components(WCC) . For WordCount, dataset used is Twitter
corpus of size 128 GB uncompressed and for WCC, a random graph of 200M
edges. Weak Scaling evaluation measured the effect of increasing both
the number of computers and the size of the input.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/scaling2.jpg}}
\caption{ Weak Scaling microbenchmark evaluation for Naiad system over
  synthetic dataset \cite{paper1-Naiad}.}
\label{weak scaling}
\end{figure}

Figure \ref{weak scaling} shows the performance of WCC on random input
graph with constant number of edges (18.2M) and nodes (9.1M) per
computer. The running time degrades significantly to 29.4s compared to
20.4s for 1.1B edge graph run on 64 computers cluster, relative to
execution in single computer. Also the WordCount application, with 2
GB compressed input per computer, doesn't achieved perfect weak
scaling, but compared to WCC, it improved.


\section{Use Cases}
Naiad framework has been majorly deployed for batch, streaming and
graph computations serving interactive queries against the results
where Naiad responds to updates and queries with sub-second latencies
\cite{paper1-Naiad}.

\subsection{Batch iterative graph computation}
Implementation of graph algorithms such as PageRank, strongly
connected components (SCC), weakly connected components(WCC), and
approximate shortest path(ASP) in Naiad requires less code complexity
and provides dominant difference in running times compared to PDW
\cite{www-PDW}, DryadLINQ \cite{paper-DryadLINQ},SHS \cite{paper-SHS}
for Category A web graph
\cite{dataset-clueweb09,paper1-Naiad,paper-Largegraphs}.
\subsection{Batch iterative machine learning} Naiad provides competitive platform for custom implementation of distributed machine learning. Also it is straightforward to build communication libraries for existing applications using Naiad’s API. For E.g., modified version of Vowpal Wabbit (VW), an open-source distributed machine learning library which performs iterative linear regression \cite{github-vowpalwabbit}. AllReduce (processes jointly performing global averaging) implementation requires 300 lines of code, around half as many as VW’s AllReduce, and the Naiad code is at a much higher level, abstracting the network sockets and threads being used \cite{paper1-Naiad}.
\subsection{Streaming acyclic computation}
Latency reduction while computing the k-exposure metric for
identifying controversial topics on Twitter using Kineograph, which
takes snapshots of continuously ingesting graph data for data parallel
computations \cite{paper-Kineograph}.
\subsection{Streaming iterative graph analytics}
Twitter Analysis: To compute the most popular hashtag in each
connected component of the graph of users mentioning other users, and
provide interactive access to the top hashtag in a user’s connected
component.

\section{Useful Resources}
Naiad: A Timely Dataflow System \cite{paper1-Naiad}, provides
extensive study of Naiad's architecture, methodology of working, its
distributed implementation, iterative and incremental data processing,
data analysis applications, comparison with other graph processing
frameworks. Moreover, \cite{github-Naiad}, \cite{www-Nuget},
\cite{paper2-Naiad} are also good resources to learn about Naiad and
programming in Naiad Framework.

\section{Conclusion}
Naiad enrich dataflow computations with timestamps that represent
logical points in the computational process and provide the basis for
an efficient, lightweight coordination mechanism. All the above
capabilities in one package allows development of High-level
programming models on Naiad which can perform tasks as streaming data
analysis, iterative machine learning, and interactive graph
mining. Moreover, public reusable low-level programming abstractions
of Naiad allow it to outperform many other data parallel systems that
enforce a single high-level programming model.

\section*{Acknowledgements}

This work was done as part of the course "I524: Big Data and Open
Source Software Projects" at Indiana University during Spring
2017. Many thanks to Professor Gregor von Laszewski and Prof. Geoffrey
Fox at Indiana University Bloomington for their academic as well as
professional guidance. We would also like to thank Associate
Instructors for their help and support during the course.

\bibliography{references}

\end{document}
