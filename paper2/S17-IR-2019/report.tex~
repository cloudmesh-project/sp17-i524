\documentclass[9pt,twocolumn,twoside]{styles/osajnl}
\usepackage{fancyvrb}
\journal{i524} 

\title{KeystoneML}



\author[1,*]{Vasanth Methkupalli}

\affil[1]{School of Informatics and Computing, Bloomington, IN 47408, U.S.A.}

\affil[*]{Corresponding authors: mvasanthiiit@gmail.com}

\dates{Paper-2, \today}

\ociscodes{Cloud, I524}

% replace this with your url in github/gitlab
\doi{\url{https://github.com/cloudmesh/classes/blob/master/docs/source/format/report/report.pdf}}


\begin{abstract}
KeystoneML is a software framework, written in Scala, from the UC
Berkeley AMPLab designed to simplify the construction of large scale,
end-to-end, machine learning pipelines with Apache Spark. KeystoneMl
and spark.ml share many features, but however there are a few
important differences, particularly around type safety and chaining,
which lead to pipelines that are easier to construct and more
robust. KeystoneML also presents a richer set of operators than those
present in spark.ml including featurizers for images, text, and
speech, and provides several example pipelines that reproduce
state-of-the-art academic results on public data sets.
\end{abstract}

\setboolean{displaycopyright}{true}

\begin{document}

\maketitle

\section{Introduction}

MLlib’s goal is to make practical machine learning (ML) scalable and
easy. Besides new algorithms and performance improvements that are
seen in each release, a great deal of time and effort has been spent
on making MLlib easy. Similar to Spark Core, MLlib provides APIs in
three languages: Python, Java, and Scala, along with user guide and
example code, to ease the learning curve for users coming from
different backgrounds. In Apache Spark 1.2, Databricks, jointly with
AMPLab, UC Berkeley, continues this effort by introducing a pipeline
API to MLlib for easy creation and tuning of practical ML pipelines.

A practical ML pipeline often involves a sequence of data
pre-processing, feature extraction, model fitting, and validation
stages. For example, classifying text documents might involve text
segmentation and cleaning, extracting features, and training a
classification model with cross-validation. Though there are many
libraries we can use for each stage, working with large-scale datasets
is not easy as it looks. Most ML libraries are not designed for
distributed computation or they do not provide native support for
pipeline creation and tuning. Unfortunately, this problem is often
ignored in academia, and it has received largely ad-hoc treatment in
industry, where development tends to occur in manual one-off pipeline
implementations.


\section{Why KeystoneML?}

KeystoneML makes constructing even complicated machine learning
pipelines easy. Here’s an example text categorization pipeline which
creates bigram features and creates a Naive Bayes model based on the
100,000 most common features.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/1}}
\caption{Code for Naive Bayes model}
\label{fig:Code for Naive Bayes model}
\end{figure}


Parallelization of the pipeline fitting
process is handled automatically and pipeline nodes are designed to
scale horizontally. Once the pipeline has been defined you can apply
it to test data and evaluate its effectiveness.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/2}}
\caption{Code for testing the data for effectiveness}
\label{fig:Code for testing the data for effectiveness}
\end{figure}


The result of this code is as follows:

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/3}}
\caption{Output for the above code}
\label{fig:Output for the above code}
\end{figure}

This relatively simple pipeline predicts the right document category
over 80 percent of the time on the test set. KeystoneML works with
much more than just text. KeystoneML is alpha software, in a very
early public release (v0.2). The project is still very young, but it
has reached a point where it is viable for general use.

\section{Linking}

KeystoneML is available from Maven Central. It can be used in our
applications by adding the following lines to the SBT project
definition:

libraryDependencies += "edu.berkeley.cs.amplab" % "keystoneml_2.10" % "0.3.0"



\section{Building}

KeystineMl is available on GitHub.

\$ git clone https://github.com/amplab/keystone.git

Once downloaded, KeystoneML can be built using the following commands:

\$ cd keystone

\$ git checkout branch-v0.3

\$ sbt/sbt assembly

\$ make

You can then run example pipelines with the included
bin/run-pipeline.sh script, or pass as an argument to spark-submit.

\section{Running and Example}

Once you’ve built KeystoneML, you can run many of the example
pipelines locally. However, to run the larger examples, you’ll want
access to a Spark cluster.

Here’s an example of running a handwriting recognition pipeline on the
popular MNIST dataset. This should be able to run on a single
machine in under a minute.


To run on a cluster, it is recommend using the spark-ec2 to launch a cluster and provision with correct versions of BLAS and native C libraries used by KeystoneML.

More scripts have been provided to set up a well-configured cluster
automatically in bin/pipelines-ec2.sh.

\subsection{Sample Figure}

Figure \ref{fig:false-color} shows an example figure.

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/sample}}
\caption{False-color image, where each pixel is assigned to one of seven reference spectra.}
\label{fig:false-color}
\end{figure}

\subsection{Sample Table}

Table \ref{tab:shape-functions} shows an example table.

\begin{table}[htbp]
\centering
\caption{\bf Shape Functions for Quadratic Line Elements}
\begin{tabular}{ccc}
\hline
local node & $\{N\}_m$ & $\{\Phi_i\}_m$ $(i=x,y,z)$ \\
\hline
$m = 1$ & $L_1(2L_1-1)$ & $\Phi_{i1}$ \\
$m = 2$ & $L_2(2L_2-1)$ & $\Phi_{i2}$ \\
$m = 3$ & $L_3=4L_1L_2$ & $\Phi_{i3}$ \\
\hline
\end{tabular}
  \label{tab:shape-functions}
\end{table}

\section{Sample Equation}

Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and
identically distributed random variables with $\text{E}[X_i] = \mu$
and $\text{Var}[X_i] = \sigma^2 < \infty$, and let

\begin{equation}
S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i
\label{eq:refname1}
\end{equation}

denote their mean. Then as $n$ approaches infinity, the random
variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal
$\mathcal{N}(0, \sigma^2)$. 

\section{Sample Algorithm}

Algorithms can be included using the commands as shown in algorithm
\ref{alg:euclid}.

\begin{algorithm}
\caption{Euclid’s algorithm}\label{alg:euclid}
\begin{algorithmic}[1]
\Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
\State $r\gets a\bmod b$
\While{$r\not=0$}\Comment{We have the answer if r is 0}
\State $a\gets b$
\State $b\gets r$
\State $r\gets a\bmod b$
\EndWhile\label{euclidendwhile}
\State \textbf{return} $b$\Comment{The gcd is b}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Python example}\label{alg:python}
\begin{quote}
\begin{Verbatim}[numbers=left]
for i in range(0,100):
  print i
\end{Verbatim}
\end{quote}
\end{algorithm}

\section{Reference Management}

The best programs to manage your references is jabref or emacs. You
can edit the references and verify them with them for format
errors. To cite them use the citation key. You can add multiple bib
files to the bibliography command separated by comma.

\noindent Add citations with the cite command. See
\cite{las14cloudmeshmultiple} for an example on how to use multiple
clouds. In \cite{www-i524} we list the class content.

Here a test of a citation with an underscore in the url \cite{www-underscore}.

\section{Supplemental Material}

You can include an appendix with important information and additional
figures if needed. However they must be referenced and follow the same
guidelines as in the main text.  All materials must be associated with
a figure, table, or equation or be referenced in the results section
of the manuscript.  (1) 2D and 3D image files and video must be
labeled “Visualization,” not “Movie,” “Video,” “Figure,” etc.  (2)
Machine-readable data (for example, csv files) must be labeled “Data
File.”  Number data files and visualizations consecutively, e.g.,
“Visualization 1, Visualization 2….”  (3) Large datasets or code files
must be placed in github/gitlab.  Such items should be mentioned in
the text as either “Dataset” or “Code,” as appropriate, and also be
cited in the references list. Appropriate citations in jabref as Misc
need to be created.

\section*{Acknowledgements}

Funding information should be listed in this section. Please evaluate
if you like to list your employer that may have funded your activities
here.  If you receive grants or project numbers, as shown in the
example.  This work was in part supported by National Science
Foundation (NSF) (1234567, 891012345) (These numbers are invented)

The acknowledgments may also contain any information that is not
related to funding:

The authors thank H. Haase, C. Wiede, and J. Gabler for technical
support.


% Bibliography

\bibliography{references}
 
\section*{Author Biographies}
\begingroup
\setlength\intextsep{0pt}
\begin{minipage}[t][3.2cm][t]{1.0\columnwidth} % Adjust height [3.2cm] as required for separation of bio photos.
  \begin{wrapfigure}{L}{0.25\columnwidth}
    \includegraphics[width=0.25\columnwidth]{images/john_smith.eps}
  \end{wrapfigure}
  \noindent
  {\bfseries John Smith} received his BSc (Mathematics) in 2000 from
  The University of Maryland. His research interests include lasers
  and optics. 
\end{minipage}
\begin{minipage}[t][3.2cm][t]{1.0\columnwidth} % Adjust height [3.2cm] as required for separation of bio photos.
  \begin{wrapfigure}{L}{0.25\columnwidth}
    \includegraphics[width=0.25\columnwidth]{images/alice_smith.eps}
  \end{wrapfigure}
  \noindent
  {\bfseries Alice Smith} received her BSc (Mathematics) in 2000 from
  The University of Maryland. Her research interests also include
  lasers and optics. 
\end{minipage}
\begin{minipage}[t][3.2cm][t]{1.0\columnwidth} % Adjust height [3.2cm] as required for separation of bio photos.
  \begin{wrapfigure}{L}{0.25\columnwidth}
    \includegraphics[width=0.25\columnwidth]{images/alice_smith.eps}
  \end{wrapfigure}
  \noindent
  {\bfseries Bruce Wayne} received his BSc (Aeronautics) in 2000 from
  Indiana University. His research interests include lasers and optics.
\end{minipage}
\endgroup

\newpage

\appendix

\section{Work Breakdown}

The work on this project was distributed as follows between the
authors:

\begin{description}

\item[John Smith.] Explored the deep mathematical knowledge needed for
  this paper and taught it to the other authors.

\item[Alice Smith.] She explored the world of Oz and was instrumental
  to work on the deployment of hadoop.

\item[Bruce Wayne.] He did not contribute at all to this paper and
  flew around to safe the world.  

\end{description}

\section{Report Checklist}

\begin{itemize}
\renewcommand{\labelitemi}{\scriptsize$\square$} 
\item Have you written the report in word or LaTeX in the specified
  format?
\item Have you included the report in github/lab?
\item Have you specified the names and e-mails of all team members in
  your report. E.g. the username in Canvas?
\item Have you included the HID of all team members?
\item Does the report have the project number added to it?
\item Have you included all images in native and PDF format in gitlab
  in the images folder?
\item Have you added the bibliography file in bibtex format?
\item Have you submitted an additional page that describes who did
  what in the project or report?
\item Have you spellchecked the paper?
\item Have you made sure you do not plagiarize?
\item Have you made sure that the important directories are all lower
  case and have no underscore or space in it?
\item Have you made sure that all authors have a README.rst in their
  HID github/lab repository?
\item Have you made sure that there is a README.rst in the project
  directory and that it is properly filled out?
\item Have you put a work breakdown in the document if you worked in a
  group?
\end{itemize}

\section{Possible technology paper outline}

The next sections are just some suggestions, your may want to add
sections and subsections as you see fit. Images and references do not
count towards the 2 page length. Please use the \verb|\section|,
\verb|\subsection|, and \verb|\subsubsection| commands in your
paper. do not introduce hardcoded numbers. Use the \verb|\ref| and
\verb|\label| commands to refer
 to the sections.


\paragraph{Abstract}

Put in the abstract a summary what this paper is about

\paragraph{1. Introduction}

Introduce the technology and provide general useful information.

\paragraph{2. Architecture} 

If applicable include a description about architectural details. This
may include a figure. Make sure that if you copy a figure you put the
\cite{?} in the caption also. Otherwise it is plagiarism.

\paragraph{2.1. API}

comment on the API which could include language bindings

\paragraph{2.2. Shell Access}

If applicable comment on how the tool can be used from the command line

\paragraph{2.3.Graphical Interface}

If applicable comment on if the technology has a GUI

\paragraph{3. Licensing}

Often tools may have different versions, some fre, some for
pay. Comment on this. For example while a tool may offer a comercial
version this version may be too costly for others. Identify especially
the difference between features for free vs commercial tools.

Sometimes you may need to introduce this also in the introduction as
there may be a big difference and without the knowledge you do not
provide the user an adequate introduction.

\paragraph{4. Ecosystem}

Some technologies have a large ecosystem developed around them with
extensions plugins and other useful tools. Identify if they exists and
comment on what they can achieve

provide potentially a mindmap or a figure illustrating how the
technology fits in with other technologies if applicable.

\paragraph{4. Use Cases}

\paragraph{4.1. Use Cases  for Big Data}

Locate and describe major usecases that demonstrate the technology
while focussing on big data related use cases. Make sure you do proper
references with the \cite{?} command. Do not put URLs in the text.

\paragraph {4.2. Other Use Cases}

Some technologies may not just be used for big data, find other makor
use cases from other areas if applicable.  Make sure you do proper
references with the \cite{?} command. Do not put URLs in the text.

\paragraph{5. Educational material}

Put information here how someone would find out more about the
technology. Use important material and do not list hundrets of web
pages, be selective.

\paragraph{6. Conclusion}

Put in some conclusion based on what you have researched

\paragraph{Acknowledgement}

Put in the information for this class and who may sponsor
you. Examples will be given later

\end{document}
